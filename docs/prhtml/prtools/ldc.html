<html><body bgcolor="#3030a0"><table bgcolor="#eeeeee"  border="5" cellspacing="0" cellpadding="5" width="700" white-space="normal"  align="center"><tr><td><table border="0" cellspacing="0" cellpadding="3" width="100%" align="center"><tr><td><a href="../prtools.html">PRTools Contents</a></td><td><p align="right"><a href="http://37steps.com/prtools/" target="_top">PRTools User Guide</a></p></td></tr></table><head><title>ldc</title></head>
<p align="center"><font face="Courier" size=5><strong>LDC</strong></font></p>
<h3> Linear Bayes Normal Classifier (BayesNormal_1)
</h3>
<p>
</p>
<p><font face="Courier" size="2">&nbsp;&nbsp;&nbsp;[W,R,S,M] = LDC(A,R,S,M)</font><br>
<font face="Courier" size="2">&nbsp;&nbsp;&nbsp;[W,R,S,M] = A*LDC([],R,S,M);</font><br>
<font face="Courier" size="2">&nbsp;&nbsp;&nbsp;[W,R,S,M] = A*LDC(R,S,M);</font><br>
</p>
<p></p><p>
<table cellspacing="0" cellpadding="3" width="100%" align="center" border="0"<tr> <td width="100"><font size="4"><strong>Input</strong></font></td><tr> <td width="120" valign="baseline"><font face="Courier" size="2">&nbsp;A</font></td><td valign="baseline"><font face="Courier" size="2">     </font>Dataset<tr> <td width="120" valign="baseline"><font face="Courier" size="2">&nbsp;R,S</font></td><td valign="baseline"><font face="Courier" size="2">   </font>Regularisation parameters<font face="Courier" size="2">, 0 <= R,S <= 1 </font> (optional; default: no regularisation, i.e.<font face="Courier" size="2"> R,S = 0) </font><tr> <td width="120" valign="baseline"><font face="Courier" size="2">&nbsp;M</font></td><td valign="baseline"><font face="Courier" size="2">     </font>Dimension of subspace structure in covariance matrix (default:<font face="Courier" size="2"> K</font>,&nbsp; all dimensions)</td></tr></table>
</p><p><table cellspacing="0" cellpadding="3" width="100%" align="center" border="0"<tr> <td width="120"><font size="4"><strong>Output</strong></font></td><tr> <td width="120" valign="baseline"><font face="Courier" size="2">&nbsp;W</font></td><td valign="baseline"><font face="Courier" size="2">   </font>Linear Bayes Normal Classifier mapping<tr> <td width="120" valign="baseline"><font face="Courier" size="2">&nbsp;R</font></td><td valign="baseline"><font face="Courier" size="2">   </font>Value of regularisation parameter<font face="Courier" size="2"> R </font>as used<tr> <td width="120" valign="baseline"><font face="Courier" size="2">&nbsp;S</font></td><td valign="baseline"><font face="Courier" size="2">   </font>Value of regularisation parameter<font face="Courier" size="2"> S </font>as used<tr> <td width="120" valign="baseline"><font face="Courier" size="2">&nbsp;M</font></td><td valign="baseline"><font face="Courier" size="2">   </font>Value of regularisation parameter<font face="Courier" size="2"> M </font>as usedd</td></tr></table>
</p><h3> Description</h3><p> Computation of the linear classifier between the classes of the dataset<font face="Courier" size="2"> A </font> by assuming normal densities with equal covariance matrices. The joint&nbsp; covariance matrix is the weighted (by a priori probabilities) average of&nbsp; the class covariance matrices.<font face="Courier" size="2"> R </font>and<font face="Courier" size="2"> S (0 <= R,S <= 1) </font>are regularisation&nbsp; parameters used for finding the covariance matrix<font face="Courier" size="2"> G </font>by<br></p>
<p><font face="Courier" size="2">&nbsp;&nbsp;&nbsp;&nbsp;G&nbsp;=&nbsp;(1-R-S)*G&nbsp;+&nbsp;R*diag(diag(G))&nbsp;+&nbsp;S*mean(diag(G))*eye(size(G,1))
</font><br></p>
<p> This covariance matrix is then decomposed as</p>
<p><font face="Courier" size="2">&nbsp;&nbsp;&nbsp;&nbsp;G&nbsp;=&nbsp;W*W'&nbsp;+&nbsp;sigma^2&nbsp;*&nbsp;eye(K)
</font><br></p>
<p> where<font face="Courier" size="2"> W </font>is a<font face="Courier" size="2"> K </font>x<font face="Courier" size="2"> M </font>matrix containing the<font face="Courier" size="2"> M </font>leading principal components&nbsp; and sigma^2 is the mean of the<font face="Courier" size="2"> K-</font>M smallest eigenvalues. The use of soft labels&nbsp; is supported. The classification<font face="Courier" size="2"> A*W </font>is computed by<font face="Courier" size="2"> NORMAL_MAP</font>.</p>
<p> If<font face="Courier" size="2"> R, S </font>or<font face="Courier" size="2"> M </font>is <font face="Courier" size="2">NaN</font> the regularisation parameter is optimised by<font face="Courier" size="2"> REGOPTC</font>.&nbsp; The best result are usually obtained by<font face="Courier" size="2"> R = 0, S = </font><font face="Courier" size="2">NaN</font><font face="Courier" size="2">, M = []</font>, or by&nbsp;<font face="Courier" size="2"> R = 0, S = 0, M = </font><font face="Courier" size="2">NaN</font> (which is for problems of moderate or low dimensionality&nbsp; faster). If no regularisation is supplied a pseudo-inverse of the&nbsp; covariance matrix is used in case it is close to singular.</p>
<p> Note that<font face="Courier" size="2"> A*(KLMS([],N)*NMC) </font>performs a similar operation by first&nbsp; pre-whitening the data in an<font face="Courier" size="2"> N-</font>dimensional space, followed by the&nbsp; nearest mean classifier. The regularisation controlled by<font face="Courier" size="2"> N </font>is different&nbsp; from the above in<font face="Courier" size="2"> LDC </font>as it entirely removes small variance directions.</p>
<p> To some extend<font face="Courier" size="2"> LDC </font>is also similar to<font face="Courier" size="2"> FISHERC</font>.</p><h3> Example(s)</h3><p>
</p><font face="Courier" size="3"><a href="../prtools/prex_plotc.html">prex_plotc</a>, </font><p><font face="Courier" size="2">&nbsp;a&nbsp;=&nbsp;gendatd;&nbsp;&nbsp;%&nbsp;generate&nbsp;Gaussian&nbsp;distributed&nbsp;data&nbsp;in&nbsp;two&nbsp;classes
</font><br><font face="Courier" size="2">&nbsp;w&nbsp;=&nbsp;ldc(a);&nbsp;&nbsp;&nbsp;%&nbsp;compute&nbsp;a&nbsp;linear&nbsp;classifier&nbsp;between&nbsp;the&nbsp;classes
</font><br><font face="Courier" size="2">&nbsp;scatterd(a);&nbsp;&nbsp;%&nbsp;make&nbsp;a&nbsp;scatterplot
</font><br><font face="Courier" size="2">&nbsp;plotc(w)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;%&nbsp;plot&nbsp;the&nbsp;classifier
</font><br></p><h3> Reference(s)</h3><p>1. R.O. Duda, P.E. Hart, and D.G. Stork, Pattern classification, 2nd edition,&nbsp;John Wiley and Sons, New York, 2001.<br>2. A. Webb, Statistical Pattern Recognition, John Wiley && Sons, New York, 2002.<br>3. C. Liu and H. Wechsler, Robust Coding Schemes for Indexing and Retrieval&nbsp;from Large Face Databases, IEEE Transactions on Image Processing, vol. 9,&nbsp;no. 1, 2000, 132-136.</p><h3> See also</h3><p>
<font face="Courier" size="3"><a href="../prtools/mappings.html">mappings</a>, <a href="../prtools/datasets.html">datasets</a>, <a href="../prtools/regoptc.html">regoptc</a>, <a href="../prtools/nmc.html">nmc</a>, <a href="../prtools/nmsc.html">nmsc</a>, <a href="../prtools/ldc.html">ldc</a>, <a href="../prtools/udc.html">udc</a>, <a href="../prtools/quadrc.html">quadrc</a>, <a href="../prtools/normal_map.html">normal_map</a>, <a href="../prtools/fisherc.html">fisherc</a>, </font></p><table border="0" cellspacing="0" cellpadding="3" width="100%" align="center"><tr><td><a href="../prtools.html">PRTools Contents</a></td><td><p align="right"><a href="http://37steps.com/prtools/" target="_top">PRTools User Guide</a></p></td></tr></table><table border="0" cellspacing="0" cellpadding="3" width="100%"><tr><td align="center"><em>This file has been automatically generated. If badly readable, use the help-command in Matlab.</em></td></tr></table></table></body></html>