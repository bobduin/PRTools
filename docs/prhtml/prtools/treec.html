<html><body bgcolor="#3030a0"><table bgcolor="#eeeeee"  border="5" cellspacing="0" cellpadding="5" width="700" white-space="normal"  align="center"><tr><td><table border="0" cellspacing="0" cellpadding="3" width="100%" align="center"><tr><td><a href="../prtools.html">PRTools Contents</a></td><td><p align="right"><a href="http://37steps.com/prtools/" target="_top">PRTools User Guide</a></p></td></tr></table><head><title>treec</title></head>
<p align="center"><font face="Courier" size=5><strong>TREEC</strong></font></p>
<h3> Trainable decision tree classifier
</h3>
<p>
</p>
<p><font face="Courier" size="2">&nbsp;&nbsp;&nbsp;&nbsp;W = TREEC(A,CRIT,PRUNE,T)</font><br>
<font face="Courier" size="2">&nbsp;&nbsp;&nbsp;&nbsp;W = A*TREEC([],CRIT,PRUNE,T)</font><br>
<font face="Courier" size="2">&nbsp;&nbsp;&nbsp;&nbsp;W = A*TREEC(CRIT,PRUNE,T)</font><br>
</p><h3> Description</h3><p> Computation of a decision tree classifier out of a dataset<font face="Courier" size="2"> A </font>using&nbsp; a binary splitting criterion<font face="Courier" size="2"> CRIT </font><br><font face="Courier" size="2"> INFCRIT  -</font>  information gain&nbsp;<font face="Courier" size="2"> MAXCRIT  -</font>  purity (default)&nbsp;<font face="Courier" size="2"> FISHCRIT -</font>  Fisher criterion</p>
<p> Pruning is defined by prune<br><font face="Courier" size="2"> PRUNE = -</font>1 pessimistic pruning as defined by Quinlan.&nbsp;<font face="Courier" size="2"> PRUNE = -</font>2 testset pruning using the dataset<font face="Courier" size="2"> T</font>, or, if not&nbsp; supplied, an artificially generated testset of<font face="Courier" size="2"> 5 </font>x size of&nbsp; the training set based on parzen density estimates.&nbsp; see<font face="Courier" size="2"> PARZENML </font>and<font face="Courier" size="2"> GENDATP</font>.&nbsp;<font face="Courier" size="2"> PRUNE = 0 </font>no pruning (default).&nbsp;<font face="Courier" size="2"> PRUNE > 0 </font>early pruning, e.g. prune<font face="Courier" size="2"> = 3 </font><font face="Courier" size="2"> PRUNE = 10 </font>causes heavy pruning.</p>
<p> If<font face="Courier" size="2"> CRIT </font>or<font face="Courier" size="2"> PRUNE </font>are set to <font face="Courier" size="2">NaN</font> they are optimised by<font face="Courier" size="2"> REGOPTC</font>.</p><h3> Reference(s)</h3><p>[1] L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone,&nbsp;Classification and regression trees, Wadsworth, California, 1984.</p><h3> See also</h3><p>
<font face="Courier" size="3"><a href="../prtools/datasets.html">datasets</a>, <a href="../prtools/mappings.html">mappings</a>, <a href="../prtools/tree_map.html">tree_map</a>, <a href="../prtools/regoptc.html">regoptc</a>, </font></p><table border="0" cellspacing="0" cellpadding="3" width="100%" align="center"><tr><td><a href="../prtools.html">PRTools Contents</a></td><td><p align="right"><a href="http://37steps.com/prtools/" target="_top">PRTools User Guide</a></p></td></tr></table><table border="0" cellspacing="0" cellpadding="3" width="100%"><tr><td align="center"><em>This file has been automatically generated. If badly readable, use the help-command in Matlab.</em></td></tr></table></table></body></html>